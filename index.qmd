---
title: "Reverse dependency checks for everyone:"
subtitle: "Automating R integration testing at cloud scale"
author: "Nan Xiao"
date: "November 17, 2025"
embed-resources: true
format:
  revealjs:
    theme: [default, custom.scss]
    mainfont: "Lato, system-ui, -apple-system, Segoe UI, Roboto, Helvetica Neue, Arial, Noto Sans, Liberation Sans, sans-serif, Apple Color Emoji, Segoe UI Emoji, Segoe UI Symbol, Noto Color Emoji"
    monofont: "JetBrains Mono, ui-monospace, SFMono-Regular, SF Mono, Menlo, Consolas, Liberation Mono, monospace"
    resources:
      - fonts/Lato-VF.woff2
      - fonts/Lato-Italic-VF.woff2
      - fonts/JetBrainsMono-Regular.woff2
    smaller: true
    slide-number: true
    scrollable: false
---

## About me

Hi, I'm Nan Xiao (@nanxstats)!

Passionate about writing fast and correct code + developer tooling to achieve that

- Creator of ggsci, pkglite, and other R and Python packages
- Work on the intersection of statistical methodology and research software engineering

## Outline

- The problem
- Background
- Scale challenges
- Solution: revdeprun
- Validation: data.table
- Lessons learned

# The problem

## What makes R unique?

- CRAN has 23,000+ packages. They all need to work together.
- When you update your package, you must check that downstream packages still work.

From [CRAN Repository Policy](https://cran.r-project.org/web/packages/policies.html):

> For a package update, please check that any packages depending on this one
> still pass `R CMD check`: it is especially expected that you will have
> checked your own packages.

From CRAN emails to maintainers after submission:

> package ggsci_4.1.0.tar.gz has been auto-processed and is pending an automated
> reverse dependency check. This service will typically respond to you within
> the next day. For technical reasons you may receive a second copy of this
> message when a team member triggers a new check.

## Why this matters

Philosophy: All packages should always work together out of the box

- Integration testing at the ecosystem level
- Python: must use virtual environments to lock and isolate dependencies
- R: CRAN tests ensure your update won't break downstream packages

Solution: invest developer time to save user time

# Background

## The process

1. Find all reverse dependencies for your package
1. Run `R CMD check` on each one, twice:
   - Once with the current CRAN version of your package
   - Once with your updated version
1. Compare results. Ensure your changes don't break them

Simple in theory. Complex in practice.

# Scale challenges

## The scale problem

- Small scale: use {tools} or {revdepcheck} or `xfun::rev_check()` manually
- Large scale: hundreds or thousands of packages to check

You need:

- High CPU count (embarrassingly parallel workload)
- Disposable environments (running untrusted code)
- Automated setup (manual setup is tedious)
- Binary packages (compiling from source takes forever)

## Existing solutions

- {revdepcheck}: mature but can't use binary repos for Linux on cloud instances
- {revdepcheck.extras}: detailed improvements but same limitations
- `xfun::rev_check()`: fast parallel checking but doesn't install system dependencies

Each solves part of the problem.

None solve the whole problem for cloud-scale checking **for everyone**.

## xkcd 927: Standards

![](images/xkcd-927.png){width=65%}

# Solution: revdeprun

## Design goals

- Target audience: CRAN package maintainers with many reverse dependencies
- Target environment: disposable Ubuntu cloud instances with high CPU count
- Easy setup: no manual R installation, no complex configuration

Run one command, get coffee, watch it work.

## The solution

- Use pak to install binary dependencies from Posit Public Package Manager (P3M)
- Use `xfun::rev_check()` for parallel checking
- Wrap everything in a Rust CLI that handles setup automatically
- One command from fresh Ubuntu to complete results

## Why Rust CLI?

- Single static binary. No runtime dependencies
- You can't use R to install R. You can't use Python to install R.
- Shell scripts get messy fast. No proper error handling, testing, or distribution.

Rust gives you:

- Proper error handling (anyhow)
- CLI parsing (clap)
- Progress bars (indicatif)
- HTTP client (reqwest)

## Installation

Fresh Ubuntu instance. Four commands:

```bash
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
sudo apt-get update && sudo apt-get install -y build-essential
cargo install revdeprun
revdeprun https://github.com/Rdatatable/data.table.git
```

That's it. Go get coffee.

## Command-line options

```bash
# Basic usage
revdeprun https://github.com/USER/PACKAGE.git

# Specify R version
revdeprun --r-version devel https://github.com/USER/PACKAGE.git

# Control parallelism
revdeprun --num-workers 48 https://github.com/USER/PACKAGE.git

# Use local directory
revdeprun ~/workspace/my-package

# Use source tarball
revdeprun ~/packages/mypackage_1.2.3.tar.gz

# Custom workspace
revdeprun --work-dir /data/workspace https://github.com/USER/PACKAGE.git
```

## What it does

- Installs the current release version of R for Ubuntu
- Discovers all reverse dependencies from CRAN metadata
- Installs system requirements for all reverse dependencies
- Pre-installs binary dependencies from P3M into `revdep/library/`
- Runs `xfun::rev_check()` with all available CPU cores
- Generates summary reports for packages with check diffs

## Why binary packages matter

- Installing thousands of packages from source would take days
- Posit Public Package Manager (P3M) provides pre-compiled binaries for Linux
- pak can install these binaries in minutes instead of hours
- This is what makes cloud-scale checking practical

## Workflow: Phase 1

Environment setup and dependency installation:

![](images/workflow-phase-1.svg){width=100%}

## Workflow: Phase 2

Parallel checking of all reverse dependencies:

![](images/workflow-phase-2.svg){width=90%}

## Key crates

- anyhow: error handling with context
- clap: command-line argument parsing
- indicatif: progress bars for long-running operations
- reqwest: HTTP client for downloading packages and metadata
- serde/serde_json: JSON parsing for package metadata
- xshell: ergonomic shell scripting in Rust

# Validation: data.table

## Why data.table?

- One of the most depended upon packages on CRAN (rank 15)
- 1,723 reverse dependencies at the time of testing
- The maintainers were kind enough to try revdeprun and report their experience
- Perfect test case for cloud-scale checking

## The instance

Cloud instance specs:

- 48 CPU cores (AMD EPYC 7713)
- 96 GB DDR4 RAM
- 2TB storage
- Ubuntu 24.04 LTS
- Cost: $1.55 per hour

## Timeline: Setup phase

00:00:00 - Started. Installing 4,395 packages required for checking.

01:48:00 - Started updating all R packages. All 48 cores at 100%.

01:55:00 - Started downloading 1,723 reverse dependencies from source.

02:12:00 - Setup complete. Time for the real work.

## Timeline: Checking phase

02:12:00 - Started checking all 1,723 packages, twice.

05:20:00 - Halfway done. 890 packages remaining.

08:55:00 - Finished. 37 packages reported check diffs.

![All 48 cores at 100% during checking](images/btop.png){width=90%}

## Results

- Total time: 8 hours 55 minutes
- Total cost: $14
- Setup time: 2 hours 12 minutes
- Checking time: 6 hours 43 minutes

For a top 20 CRAN package, less than the cost of lunch

# Lessons learned

## Developer tools matter

- Good developer tools make developers productive.
- Automation saves time. Time saves money. Money enables more development.
- The R community has excellent package development tools.
- Commoditize cloud-scale automation is the future.

## Complexity is real

- Multiple people tried solving this problem.
- Each solution tackles part of it.
- If many smart people couldn't solve it completely, it's genuinely hard.
- The devil is in the details: system dependencies, binary repos, parallel checking.

## What to optimize

- In the age of AI-assisted coding, the question shifts.
- Not: how do you build something?
- But: what should you build?
- Choose problems that matter. Build tools that save time at scale.

## Security matters

**Never run reverse dependency checks on your local machine**

- Reverse dependency checks execute arbitrary third-party code
- Always use disposable cloud instances or containers
- Destroy them when done

## Future directions

- What about packages with 10,000+ reverse dependencies?
- Can we optimize the 2+ hour setup phase? `rv`?
- Can we cache dependency installations across runs?
- How do we handle Bioconductor packages?

These are open questions. Contributions welcome.

# Questions?

## Resources

**revdeprun on GitHub**

<https://github.com/nanxstats/revdeprun>

**revdeprun on crates.io**

<https://crates.io/crates/revdeprun>

**Blog posts**

Announcing revdeprun: <https://nanx.me/blog/post/revdeprun/>

data.table speedrun: <https://nanx.me/blog/post/revdep-speedrun/>
